{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression & Perceptron Comparison on Sonar Dataset by Aana Kakroo (20BAI1138)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required Python libraries  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0200</th>\n",
       "      <th>0.0371</th>\n",
       "      <th>0.0428</th>\n",
       "      <th>0.0207</th>\n",
       "      <th>0.0954</th>\n",
       "      <th>0.0986</th>\n",
       "      <th>0.1539</th>\n",
       "      <th>0.1601</th>\n",
       "      <th>0.3109</th>\n",
       "      <th>0.2111</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0027</th>\n",
       "      <th>0.0065</th>\n",
       "      <th>0.0159</th>\n",
       "      <th>0.0072</th>\n",
       "      <th>0.0167</th>\n",
       "      <th>0.0180</th>\n",
       "      <th>0.0084</th>\n",
       "      <th>0.0090</th>\n",
       "      <th>0.0032</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109  \\\n",
       "0    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "1    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "2    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "3    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "4    0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "202  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "203  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "204  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "205  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "206  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "     0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084  \\\n",
       "0    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "1    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "2    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "3    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "4    0.3039  ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "202  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "203  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "204  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "205  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "206  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "     0.0090  0.0032  R  \n",
       "0    0.0052  0.0044  R  \n",
       "1    0.0095  0.0078  R  \n",
       "2    0.0040  0.0117  R  \n",
       "3    0.0107  0.0094  R  \n",
       "4    0.0051  0.0062  R  \n",
       "..      ...     ... ..  \n",
       "202  0.0193  0.0157  M  \n",
       "203  0.0062  0.0067  M  \n",
       "204  0.0077  0.0031  M  \n",
       "205  0.0036  0.0048  M  \n",
       "206  0.0061  0.0115  M  \n",
       "\n",
       "[207 rows x 61 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing datasets  \n",
    "df= pd.read_csv(r\"C:\\Users\\aanak\\OneDrive\\Desktop\\Materials for college\\ML\\LAB\\sonar.csv\") \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 207 entries, 0 to 206\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0.0200  207 non-null    float64\n",
      " 1   0.0371  207 non-null    float64\n",
      " 2   0.0428  207 non-null    float64\n",
      " 3   0.0207  207 non-null    float64\n",
      " 4   0.0954  207 non-null    float64\n",
      " 5   0.0986  207 non-null    float64\n",
      " 6   0.1539  207 non-null    float64\n",
      " 7   0.1601  207 non-null    float64\n",
      " 8   0.3109  207 non-null    float64\n",
      " 9   0.2111  207 non-null    float64\n",
      " 10  0.1609  207 non-null    float64\n",
      " 11  0.1582  207 non-null    float64\n",
      " 12  0.2238  207 non-null    float64\n",
      " 13  0.0645  207 non-null    float64\n",
      " 14  0.0660  207 non-null    float64\n",
      " 15  0.2273  207 non-null    float64\n",
      " 16  0.3100  207 non-null    float64\n",
      " 17  0.2999  207 non-null    float64\n",
      " 18  0.5078  207 non-null    float64\n",
      " 19  0.4797  207 non-null    float64\n",
      " 20  0.5783  207 non-null    float64\n",
      " 21  0.5071  207 non-null    float64\n",
      " 22  0.4328  207 non-null    float64\n",
      " 23  0.5550  207 non-null    float64\n",
      " 24  0.6711  207 non-null    float64\n",
      " 25  0.6415  207 non-null    float64\n",
      " 26  0.7104  207 non-null    float64\n",
      " 27  0.8080  207 non-null    float64\n",
      " 28  0.6791  207 non-null    float64\n",
      " 29  0.3857  207 non-null    float64\n",
      " 30  0.1307  207 non-null    float64\n",
      " 31  0.2604  207 non-null    float64\n",
      " 32  0.5121  207 non-null    float64\n",
      " 33  0.7547  207 non-null    float64\n",
      " 34  0.8537  207 non-null    float64\n",
      " 35  0.8507  207 non-null    float64\n",
      " 36  0.6692  207 non-null    float64\n",
      " 37  0.6097  207 non-null    float64\n",
      " 38  0.4943  207 non-null    float64\n",
      " 39  0.2744  207 non-null    float64\n",
      " 40  0.0510  207 non-null    float64\n",
      " 41  0.2834  207 non-null    float64\n",
      " 42  0.2825  207 non-null    float64\n",
      " 43  0.4256  207 non-null    float64\n",
      " 44  0.2641  207 non-null    float64\n",
      " 45  0.1386  207 non-null    float64\n",
      " 46  0.1051  207 non-null    float64\n",
      " 47  0.1343  207 non-null    float64\n",
      " 48  0.0383  207 non-null    float64\n",
      " 49  0.0324  207 non-null    float64\n",
      " 50  0.0232  207 non-null    float64\n",
      " 51  0.0027  207 non-null    float64\n",
      " 52  0.0065  207 non-null    float64\n",
      " 53  0.0159  207 non-null    float64\n",
      " 54  0.0072  207 non-null    float64\n",
      " 55  0.0167  207 non-null    float64\n",
      " 56  0.0180  207 non-null    float64\n",
      " 57  0.0084  207 non-null    float64\n",
      " 58  0.0090  207 non-null    float64\n",
      " 59  0.0032  207 non-null    float64\n",
      " 60  R       207 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 98.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['R'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the inputs (first 59 columns) and targets (column 60 dummie data) for then use it as the model inputs and outputs.\n",
    "First the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0200</th>\n",
       "      <th>0.0371</th>\n",
       "      <th>0.0428</th>\n",
       "      <th>0.0207</th>\n",
       "      <th>0.0954</th>\n",
       "      <th>0.0986</th>\n",
       "      <th>0.1539</th>\n",
       "      <th>0.1601</th>\n",
       "      <th>0.3109</th>\n",
       "      <th>0.2111</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0232</th>\n",
       "      <th>0.0027</th>\n",
       "      <th>0.0065</th>\n",
       "      <th>0.0159</th>\n",
       "      <th>0.0072</th>\n",
       "      <th>0.0167</th>\n",
       "      <th>0.0180</th>\n",
       "      <th>0.0084</th>\n",
       "      <th>0.0090</th>\n",
       "      <th>0.0032</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109  \\\n",
       "0  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "1  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "2  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "3  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "4  0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
       "\n",
       "   0.2111  ...  0.0232  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  \\\n",
       "0  0.2872  ...  0.0125  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140   \n",
       "1  0.6194  ...  0.0033  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316   \n",
       "2  0.1264  ...  0.0241  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050   \n",
       "3  0.4459  ...  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   \n",
       "4  0.3039  ...  0.0104  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057   \n",
       "\n",
       "   0.0084  0.0090  0.0032  \n",
       "0  0.0049  0.0052  0.0044  \n",
       "1  0.0164  0.0095  0.0078  \n",
       "2  0.0044  0.0040  0.0117  \n",
       "3  0.0048  0.0107  0.0094  \n",
       "4  0.0027  0.0051  0.0062  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.drop('R', axis=1)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding for the data in the 'R' column (target variable/output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import label encoder\n",
    "from sklearn import preprocessing\n",
    " \n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    " \n",
    "# Encode labels in column 'R'.\n",
    "df['R']= label_encoder.fit_transform(df['R'])\n",
    " \n",
    "y = df['R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "202    0\n",
       "203    0\n",
       "204    0\n",
       "205    0\n",
       "206    0\n",
       "Name: R, Length: 207, dtype: int32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split data in 60:40 (60% for training and 40% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Logistic Regression Model (taking penalty=l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import LogisticRegression class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier1 = LogisticRegression(penalty='l2',random_state = 0)\n",
    "classifier1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do prediction using trained model\n",
    "y_pred1 = classifier1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[36  9]\n",
      " [ 6 32]]\n"
     ]
    }
   ],
   "source": [
    "#import class for confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#get confusion matrix\n",
    "cm1 = confusion_matrix(y_test, y_pred1)\n",
    " \n",
    "#print confusion matrix\n",
    "print (\"Confusion Matrix : \\n\", cm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8192771084337349\n"
     ]
    }
   ],
   "source": [
    "#import class for accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#print accuracy score of model\n",
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Logistic Regression Model (taking penalty=none, increasing max iterations from 100 to 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=150, penalty='none', random_state=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2 = LogisticRegression(penalty='none',max_iter=150,random_state = 0)\n",
    "classifier2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = classifier2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[35 10]\n",
      " [ 7 31]]\n"
     ]
    }
   ],
   "source": [
    "cm2 = confusion_matrix(y_test, y_pred2)\n",
    " \n",
    "print (\"Confusion Matrix : \\n\", cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7951807228915663\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Logistic Regression Model (taking penalty=l1, changing solver from ‘lbfgs’ to ‘liblinear’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='l1', random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier3 = LogisticRegression(penalty='l1',solver='liblinear',random_state = 0)\n",
    "classifier3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = classifier3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[36  9]\n",
      " [ 8 30]]\n"
     ]
    }
   ],
   "source": [
    "cm3 = confusion_matrix(y_test, y_pred3)\n",
    " \n",
    "print (\"Confusion Matrix : \\n\", cm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7951807228915663\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Logistic Regression Model (taking penalty='elasticnet', keeping l1 ratio = 0.2, increasing max iterations to 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[36  9]\n",
      " [ 6 32]]\n",
      "Accuracy :  0.8192771084337349\n"
     ]
    }
   ],
   "source": [
    "classifier4 = LogisticRegression(penalty='elasticnet',max_iter=200,l1_ratio=0.2,solver='saga',random_state = 0)\n",
    "classifier4.fit(X_train, y_train)\n",
    "\n",
    "y_pred4 = classifier4.predict(X_test)\n",
    "\n",
    "cm4 = confusion_matrix(y_test, y_pred4)\n",
    "print (\"Confusion Matrix : \\n\", cm4)\n",
    "\n",
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Logistic Regression Model (taking penalty='elasticnet', keeping l1 ratio = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[35 10]\n",
      " [ 9 29]]\n",
      "Accuracy :  0.7710843373493976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanak\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier5 = LogisticRegression(penalty='elasticnet',l1_ratio=0.6,solver='saga',random_state = 0)\n",
    "classifier5.fit(X_train, y_train)\n",
    "\n",
    "y_pred5 = classifier5.predict(X_test)\n",
    "\n",
    "cm5 = confusion_matrix(y_test, y_pred5)\n",
    "print (\"Confusion Matrix : \\n\", cm5)\n",
    "\n",
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, the highest accuracy score obtained from Logistic Regression Model from the given dataset is 0.8192771084337349 which we achieved by taking penalty = l2, random state = 0 and by taking penalty='elasticnet', keeping l1 ratio = 0.2, increasing max iterations to 200.\n",
    "Thus, the above mentioned models are most suited to be deployed on the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7681159420289855"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Perceptron class\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "#build model\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(x, y)\n",
    "clf.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking penalty=l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8067632850241546"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = Perceptron(penalty='l1',tol=1e-3, random_state=0)\n",
    "clf1.fit(x, y)\n",
    "clf1.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking penalty=l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7632850241545893"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = Perceptron(penalty='l2',tol=1e-3, random_state=0)\n",
    "clf2.fit(x, y)\n",
    "clf2.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking penalty=elasticnet and l1 ratio=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8599033816425121"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = Perceptron(penalty='elasticnet',l1_ratio=0.2,tol=1e-3, random_state=0)\n",
    "clf3.fit(x, y)\n",
    "clf3.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking same parameters as above but changing class weight to balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855072463768116"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = Perceptron(penalty='elasticnet',l1_ratio=0.2,tol=1e-3, random_state=0,class_weight='balanced')\n",
    "clf4.fit(x, y)\n",
    "clf4.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the highest possible accuracy score obtained is 0.8599033816425121 when taking parameters as penalty='elasticnet',l1_ratio=0.2,tol=1e-3, random_state=0. Hence, the mentioned perceptron model is the most suited for the given sonar dataset."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5c45ef5b26bc75a25a693b192e8e30504c0d9e3491665f81bc3bb4a71b48eb9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
